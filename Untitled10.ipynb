{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YoHsQ3kGCR1EJF3rVFwqRhmnTLQi1Cw1",
      "authorship_tag": "ABX9TyPer/zQFVhqbumA/HwHd7W7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hidhariyas/AICT-intership2025/blob/main/Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1lxOA5yfmbZ",
        "outputId": "f1a0984d-a15e-42c4-dacb-f2509239f396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2527 files belonging to 6 classes.\n",
            "Using 2022 files for training.\n",
            "Found 2527 files belonging to 6 classes.\n",
            "Using 505 files for validation.\n",
            "Epoch 1/15\n",
            "\u001b[1m38/64\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 2s/step - accuracy: 0.1995 - loss: 1.8183"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Rescaling, GlobalAveragePooling2D\n",
        "from tensorflow.keras import layers, optimizers, callbacks\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.applications import EfficientNetV2B2\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
        "import gradio as gr\n",
        "import os\n",
        "import json\n",
        "\n",
        "# === PATH SETUP ===\n",
        "dataset_dir = r\"/content/drive/MyDrive/dataset_unzipped/TrashType_Image_Dataset\"\n",
        "image_size = (124, 124)\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "# === DATASET LOADING ===\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir, validation_split=0.2, subset=\"training\",\n",
        "    seed=seed, shuffle=True, image_size=image_size, batch_size=batch_size)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir, validation_split=0.2, subset=\"validation\",\n",
        "    seed=seed, shuffle=True, image_size=image_size, batch_size=batch_size)\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Cache & Prefetch\n",
        "train_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Split test set\n",
        "val_batches = tf.data.experimental.cardinality(val_ds)\n",
        "test_ds = val_ds.take(val_batches // 2).cache().prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.skip(val_batches // 2)\n",
        "\n",
        "# === DATA VISUALIZATION (Optional) ===\n",
        "def show_sample(dataset):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for images, labels in dataset.take(1):\n",
        "        for i in range(min(12, len(images))):\n",
        "            ax = plt.subplot(4, 3, i + 1)\n",
        "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "            plt.title(class_names[labels[i]])\n",
        "            plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# show_sample(train_ds)  # Uncomment if needed\n",
        "\n",
        "# === DISTRIBUTION + CLASS WEIGHTS ===\n",
        "def count_distribution(dataset):\n",
        "    total = 0\n",
        "    counts = {k: 0 for k in class_names}\n",
        "    for _, labels in dataset:\n",
        "        for label in labels.numpy():\n",
        "            counts[class_names[label]] += 1\n",
        "            total += 1\n",
        "    return {k: round(v / total * 100, 2) for k, v in counts.items()}\n",
        "\n",
        "def simple_bar_plot(dist, title):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(dist.keys(), dist.values(), color='mediumseagreen')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Percentage (%)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "train_dist = count_distribution(train_ds)\n",
        "# simple_bar_plot(train_dist, \"Training Set Distribution (%)\")  # Optional\n",
        "\n",
        "# Compute class weights\n",
        "label_list = []\n",
        "for _, labels in train_ds:\n",
        "    label_list.extend(labels.numpy())\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.arange(num_classes), y=label_list)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# === MODEL CREATION ===\n",
        "data_augmentation = Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.15),\n",
        "    layers.RandomZoom(0.15),\n",
        "    layers.RandomContrast(0.15),\n",
        "    layers.RandomBrightness(0.1),\n",
        "])\n",
        "\n",
        "base_model = EfficientNetV2B2(include_top=False, weights='imagenet',\n",
        "                              input_shape=(124, 124, 3), include_preprocessing=True)\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:100]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model = Sequential([\n",
        "    layers.Input(shape=(124, 124, 3)),\n",
        "    data_augmentation,\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=optimizers.Adam(1e-4),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# === TRAINING ===\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=15,\n",
        "                    class_weight=class_weights, callbacks=[early_stop])\n",
        "\n",
        "# === EVALUATION ===\n",
        "loss, acc = model.evaluate(test_ds)\n",
        "print(f\"✅ Test Accuracy: {acc:.4f} | Loss: {loss:.4f}\")\n",
        "\n",
        "y_true = np.concatenate([y.numpy() for _, y in test_ds])\n",
        "y_pred = np.argmax(model.predict(test_ds), axis=1)\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
        "plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# === SAVE MODEL + CLASS NAMES ===\n",
        "model.save(\"EfficientNetV2B2_Trash_Classifier.keras\")\n",
        "with open(\"class_names.json\", \"w\") as f:\n",
        "    json.dump(class_names, f)\n",
        "\n",
        "# === GRADIO APP ===\n",
        "# Load everything\n",
        "loaded_model = tf.keras.models.load_model(\"EfficientNetV2B2_Trash_Classifier.keras\")\n",
        "with open(\"class_names.json\", \"r\") as f:\n",
        "    class_names = json.load(f)\n",
        "\n",
        "def classify_image(img):\n",
        "    img = img.resize((124, 124))\n",
        "    arr = np.array(img, dtype=np.float32)\n",
        "    arr = preprocess_input(arr)\n",
        "    pred = loaded_model.predict(np.expand_dims(arr, axis=0))[0]\n",
        "    pred_class = class_names[np.argmax(pred)]\n",
        "    confidence = np.max(pred)\n",
        "    return f\"Predicted: {pred_class} (Confidence: {confidence:.2f})\"\n",
        "\n",
        "iface = gr.Interface(fn=classify_image, inputs=gr.Image(type='pil'), outputs=\"text\")\n",
        "iface.launch()"
      ]
    }
  ]
}